
 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\constants.py": 
 
 
from imports import *

# constants.py
NUM_SIMULATIONS = 3
N_TRIALS = 5
NUM_EPOCHS = 20
LOOK_BACK = 156
FORECAST_RANGE = 13

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load data
end_date = datetime.now()
start_date = end_date - timedelta(days=365*5)  # 5 years of data

NUM_SIMULATIONS = 10  # or any other number you prefer
N_TRIALS = 20
NUM_EPOCHS = 50
LOOK_BACK = 156
FORECAST_RANGE = 13

# Download data for tickers
tickers = ['GOOGL', 'SPY']

# Define your parameters
forecast_features = ['stock.GOOGL.Adj Close']

features = [
    'stock.GOOGL.Adj Close', 
    'stock.GOOGL.Volume', 
    'stock.GOOGL.vwp', 
    'stock.GOOGL.p-1',
    'fred.T10Y3M.value-2',
    'fred.T10Y3M.value-91',
    'fred.EFFR.value-2',
    'fred.EFFR.value-91',
    'date.quarter.Q1',
    'date.quarter.Q2',
    'date.quarter.Q3',
    'date.quarter.Q4'
]

# Define indicators
indicators = ['T10Y3M', 'EFFR']

# Generate include flags from the features list
include_flags = [f'include.{feature}' for feature in features[1:]]  # Skip the first feature as it is always included

trial_results = []
 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\imports.py": 
 
 
# imports.py
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pandas_datareader import data as pdr
from datetime import datetime, timedelta
import yfinance as yf
from scipy.stats import ttest_ind
from sklearn.preprocessing import MinMaxScaler
import matplotlib.dates as mdates
import optuna
import functools
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from scipy import stats

 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\main.py": 
 
 
from imports import *
from constants import *
from CNN_LSTM.evaluation import objective, backtest_and_evaluate

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

data = {ticker: yf.download(ticker, start=start_date, end=end_date) for ticker in tickers}

# Fetch data for each indicator
indicator_data = {}
for indicator in indicators:
    indicator_data[indicator] = pdr.get_data_fred(indicator, start=start_date, end=end_date)
    indicator_data[indicator + '-2'] = indicator_data[indicator].shift(2).resample('W').last()
    indicator_data[indicator + '-91'] = indicator_data[indicator].shift(91).resample('W').last()

# Create a new DataFrame to hold the required features
stock_data = []
for key in data.keys():
    df = data[key]
    df['vwp'] = df['Adj Close'] * df['Volume']
    df['p-1'] = df['Adj Close'].shift(1)
    df = df[['Adj Close', 'vwp', 'p-1', 'Volume']].resample('W').last()
    df['date_feature'] = (df.index - df.index[0]).days
    df['ticker'] = key
    stock_data.append(df)

# Combine stock data into a single DataFrame
combined_stock_data = pd.concat(stock_data)

# Pivot the data to have a multi-level column index
pivot_stock_df = combined_stock_data.pivot_table(index='Date', columns='ticker', values=['Adj Close', 'vwp', 'p-1', 'Volume', 'date_feature'])
pivot_stock_df.columns = pd.MultiIndex.from_tuples([('stock', col[1], col[0]) for col in pivot_stock_df.columns])

# Create a DataFrame for the indicators with a multi-level column index
pivot_indicator_df = pd.DataFrame(index=pivot_stock_df.index)
for indicator in indicators:
    pivot_indicator_df[('fred', indicator, 'value-2')] = indicator_data[indicator + '-2']
    pivot_indicator_df[('fred', indicator, 'value-91')] = indicator_data[indicator + '-91']

# Combine stock and indicator data
combined_df = pd.concat([pivot_stock_df, pivot_indicator_df], axis=1)
combined_df.dropna(inplace=True)
# Create binary flags for Q1, Q2, Q3, and Q4 directly using index month
combined_df[('date_', 'quarter', 'Q1')] = combined_df.index.to_series().apply(lambda x: 1 if x.month in [1, 2, 3] else 0)
combined_df[('date_', 'quarter', 'Q2')] = combined_df.index.to_series().apply(lambda x: 1 if x.month in [4, 5, 6] else 0)
combined_df[('date_', 'quarter', 'Q3')] = combined_df.index.to_series().apply(lambda x: 1 if x.month in [7, 8, 9] else 0)
combined_df[('date_', 'quarter', 'Q4')] = combined_df.index.to_series().apply(lambda x: 1 if x.month in [10, 11, 12] else 0)

# Run the study
study = optuna.create_study(direction="minimize")
study.optimize(functools.partial(objective, 
                                 combined_df=combined_df, 
                                 forecast_features=forecast_features,
                                 trial_results=trial_results,
                                 LOOK_BACK=LOOK_BACK,
                                 FORECAST_RANGE=FORECAST_RANGE,
                                 NUM_EPOCHS=NUM_EPOCHS), 
               n_trials=N_TRIALS)

# Convert results to a DataFrame
results_df = pd.DataFrame(trial_results)

significant_features = []
for feature in include_flags:
    mape_with_feature = results_df[results_df[feature] == True]['mape']
    mape_without_feature = results_df[results_df[feature] == False]['mape']
    
    t_stat, p_value = stats.ttest_ind(mape_with_feature, mape_without_feature)
    
    if p_value < 0.05:  # Using 0.05 as the significance level
        significant_features.append(feature)
        print(f"{feature} is significant with p-value: {p_value}")

# Sort trials by MAPE
results_df = results_df.sort_values('trial_number')

# Create the plot
plt.figure(figsize=(12, 6))
plt.plot(results_df['trial_number'], results_df['mape'], marker='o')

# Highlight points where significant features were included
for feature in significant_features:
    feature_trials = results_df[results_df[feature] == True]
    plt.scatter(feature_trials['trial_number'], feature_trials['mape'], 
                label=feature, s=100, alpha=0.6)

plt.title('MAPE Reduction Over Optuna Trials')
plt.xlabel('Trial Number')
plt.ylabel('MAPE')
plt.legend()
plt.grid(True, alpha=0.3)

# Add text annotation for best MAPE
best_trial = results_df.loc[results_df['mape'].idxmin()]
plt.annotate(f"Best MAPE: {best_trial['mape']:.2f}", 
             xy=(best_trial['trial_number'], best_trial['mape']), 
             xytext=(5, 5), textcoords='offset points')
plt.savefig('trial_mape.png')
#plt.show()

# Print the best trial information
print("\nBest trial:")
print(f"  MAPE: {best_trial['mape']:.2f}")
print("  Features:")
for feature in include_flags:
    if best_trial[feature]:
        print(f"    - {feature}")

# The rest of your code remains unchanged
best_trial = study.best_trial
print("Best trial:")
print(f"  Value: {best_trial.value}")
print("  Params: ")
for key, value in best_trial.params.items():
    print(f"    {key}: {value}")

backtest_and_evaluate(study, combined_df, MinMaxScaler(), features, forecast_features, NUM_EPOCHS=NUM_EPOCHS)
 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\CNN_LSTM\CNN_TS.py": 
 
 
from imports import *

class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.hidden_dim = hidden_dim
        self.attention = nn.Linear(hidden_dim * 2, hidden_dim)
        self.v = nn.Parameter(torch.rand(hidden_dim))
    
    def forward(self, hidden, encoder_outputs):
        batch_size = encoder_outputs.shape[0]
        seq_len = encoder_outputs.shape[1]

        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)
        
        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))
        
        energy = energy.permute(0, 2, 1)
        v = self.v.repeat(batch_size, 1).unsqueeze(1)
        attention_weights = torch.bmm(v, energy).squeeze(1)
        attention_weights = torch.softmax(attention_weights, dim=1)
        
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)
        
        return context, attention_weights

class LSTMEncoderDecoderAttention(nn.Module):
    def __init__(self, look_back, forecast_range, n_features, hidden_dim, n_outputs):
        super(LSTMEncoderDecoderAttention, self).__init__()
        self.encoder = nn.LSTM(n_features, hidden_dim, batch_first=True)
        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        self.attention = Attention(hidden_dim)
        self.fc = nn.Linear(hidden_dim, n_outputs)
        self.hidden_dim = hidden_dim
        self.forecast_range = forecast_range

    def forward(self, x):
        encoder_outputs, (hidden, cell) = self.encoder(x)
        
        decoder_input = hidden[-1].unsqueeze(1).repeat(1, self.forecast_range, 1)
        decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))
        
        outputs = []
        for t in range(self.forecast_range):
            context, _ = self.attention(hidden[-1], encoder_outputs)
            decoder_output_t = decoder_output[:, t, :] + context
            outputs.append(self.fc(decoder_output_t))
        
        return torch.stack(outputs, dim=1)
 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\CNN_LSTM\evaluation.py": 
 
 
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler
from .training import train_model
from .CNN_TS import LSTMEncoderDecoderAttention
from imports import *
from constants import *
from .visualization import plot_predictions_with_intervals

# Function to split sequence for LSTM
def split_sequence(sequence, look_back, forecast_range, forecast_indices):
    X, y = [], []
    for i in range(len(sequence)):
        end_ix = i + look_back
        out_end_ix = end_ix + forecast_range
        if out_end_ix > len(sequence):
            break
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix, forecast_indices]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

def calculate_prediction_intervals_based_on_residuals(predictions, std_devs):
    lower_bounds = predictions - std_devs
    upper_bounds = predictions + std_devs
    return lower_bounds, upper_bounds

def inverse_transform(scaler, y_test, yhat, forecast_indices):
    y_test_reshaped = y_test.reshape(-1, y_test.shape[-1])
    yhat_reshaped = yhat.reshape(-1, yhat.shape[-1])
    
    y_test_full = np.zeros((y_test_reshaped.shape[0], len(scaler.scale_)))
    yhat_full = np.zeros((yhat_reshaped.shape[0], len(scaler.scale_)))
    
    y_test_full[:, forecast_indices] = y_test_reshaped
    yhat_full[:, forecast_indices] = yhat_reshaped
    
    y_test_inverse = scaler.inverse_transform(y_test_full)
    yhat_inverse = scaler.inverse_transform(yhat_full)
    
    y_test_inverse = y_test_inverse[:, forecast_indices]
    yhat_inverse = yhat_inverse[:, forecast_indices]
    
    return yhat_inverse, y_test_inverse

def evaluate_forecast(y_test_inverse, yhat_inverse):
    mse = np.mean((y_test_inverse - yhat_inverse) ** 2)
    mae = np.mean(np.abs(y_test_inverse - yhat_inverse))
    mape = np.mean(np.abs((y_test_inverse - yhat_inverse) / y_test_inverse)) * 100
    return mae, mse, mape

def objective(trial, combined_df, forecast_features, trial_results, LOOK_BACK, FORECAST_RANGE, NUM_EPOCHS):
    # Select features to include using trial suggestions
    selected_flags = [trial.suggest_categorical(flag, [True, False]) for flag in include_flags]
    
    # Construct the feature list based on the trial's suggestions
    selected_features = [features[0]]  # Always include the main feature
    for index, flag in enumerate(selected_flags, 1):
        if flag:
            selected_features.append(features[index])
    
    # Convert selected_features to tuples to match combined_df's multi-level column index
    selected_features = [tuple(feature.split('.')) for feature in selected_features]
    
    # Ensure that the selected features exist in the combined_df
    valid_features = [feature for feature in selected_features if feature in combined_df.columns]
    
    # Debug print statement to verify selected and valid features
    print(f"Selected features: {selected_features}")
    print(f"Valid features: {valid_features}")
    print(f"Combined_df columns: {combined_df.columns.tolist()}")

    if not valid_features:
        raise ValueError("No valid features selected for training.")
    
    # Extract and process the valid features
    data_subset = combined_df[valid_features].dropna()
    
    # Handle the case where the data subset is empty after dropping NaNs
    if data_subset.empty:
        raise ValueError("No data available after dropping NaNs.")
    
    # Scaling the data
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data_subset)
    scaled_data = pd.DataFrame(scaled_data, columns=data_subset.columns, index=data_subset.index)
    
    # Flatten the multi-index columns for easier mapping
    scaled_data.columns = ['.'.join(col) for col in scaled_data.columns]
    
    # Derive feature mapping from combined_df
    feature_mapping = {col: idx for idx, col in enumerate(scaled_data.columns)}
    
    # Map forecast features to their indices
    forecast_indices = [feature_mapping[feature] for feature in forecast_features]
    n_outputs = len(forecast_indices)
    
    mape_results = []
    for _ in range(NUM_SIMULATIONS):
        # Prepare the data for LSTM
        X, y = split_sequence(scaled_data.values, LOOK_BACK, FORECAST_RANGE, forecast_indices)
        
        if X.size == 0 or y.size == 0:
            return float('inf')  # Return an invalid score if there's no data
        
        X_train = torch.tensor(X, dtype=torch.float32).to(device)
        y_train = torch.tensor(y, dtype=torch.float32).to(device)
        
        n_features = X_train.shape[2] if X_train.ndimension() > 2 else 0
        
        # Define the model
        hidden_dim = 24
        
        model = LSTMEncoderDecoderAttention(LOOK_BACK, FORECAST_RANGE, n_features, hidden_dim, n_outputs).to(device)
        
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        batch_size = 32
        early_stopping_patience = 10
        
        train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
        
        train_model(model, train_loader, criterion, optimizer, NUM_EPOCHS, early_stopping_patience)
        
        model.load_state_dict(torch.load('best_model.pth', map_location=device))
        
        model.eval()
        with torch.no_grad():
            yhat = model(X_train).cpu().numpy()
        
        yhat_inverse, y_train_inverse = inverse_transform(scaler, y_train.cpu().numpy(), yhat, forecast_indices)
        
        mae, mse, mape = evaluate_forecast(y_train_inverse, yhat_inverse)
        mape_results.append(mape)
    
    average_mape = np.mean(mape_results)
    
    # Store the results
    trial_results.append({
        'trial_number': trial.number,
        'mape': average_mape,
        **{include_flags[i-1]: selected_flags[i-1] for i in range(1, len(features))}
    })
    
    return average_mape

def backtest_and_evaluate(study, combined_df, scaler, features, forecast_features, LOOK_BACK=156, FORECAST_RANGE=13, NUM_EPOCHS=20):
    # Convert features and forecast_features to tuples to match combined_df's multi-level column index
    features = [tuple(feature.split('.')) for feature in features]
    forecast_features = [tuple(feature.split('.')) for feature in forecast_features]
    
    # Select valid features from combined_df
    selected_columns = [col for col in combined_df.columns if col in features]
    
    # Debug print statements to check selected columns and data_subset
    print(f"Features: {features}")
    print(f"Selected columns: {selected_columns}")
    
    data_subset = combined_df[selected_columns].dropna()
    
    # Debug print statement to check data_subset
    print(f"Data subset shape: {data_subset.shape}")
    print(f"Data subset head:\n{data_subset.head()}")
    
    if data_subset.empty:
        raise ValueError("No data available after dropping NaNs.")
    
    scaled_data = scaler.fit_transform(data_subset)
    scaled_data = pd.DataFrame(scaled_data, columns=data_subset.columns, index=data_subset.index)
    
    # The rest of your function remains unchanged
    scaled_data.columns = ['.'.join(col) if isinstance(col, tuple) else col for col in scaled_data.columns]
    feature_mapping = {col: idx for idx, col in enumerate(scaled_data.columns)}
    
    forecast_features = ['.'.join(feature) if isinstance(feature, tuple) else feature for feature in forecast_features]
    forecast_indices = [feature_mapping[feature] for feature in forecast_features]
    
    X, y = split_sequence(scaled_data.values, LOOK_BACK, FORECAST_RANGE, forecast_indices)
    
    if X.size == 0 or y.size == 0:
        return
    
    X_train = torch.tensor(X, dtype=torch.float32).to(device)
    y_train = torch.tensor(y, dtype=torch.float32).to(device)
    
    hidden_dim = 24
    n_features = X_train.shape[2] if X_train.ndimension() > 2 else 0
    
    model = LSTMEncoderDecoderAttention(LOOK_BACK, FORECAST_RANGE, n_features, hidden_dim, len(forecast_indices)).to(device)
    
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    batch_size = 32
    early_stopping_patience = 10
    
    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    
    train_model(model, train_loader, criterion, optimizer, NUM_EPOCHS, early_stopping_patience)
    
    model.load_state_dict(torch.load('best_model.pth', map_location=device))
    
    model.eval()
    with torch.no_grad():
        yhat = model(X_train).cpu().numpy()
    
    yhat_inverse, y_train_inverse = inverse_transform(scaler, y_train.cpu().numpy(), yhat, forecast_indices)
    
    y_train_inverse_time_step = y_train_inverse.reshape(-1, FORECAST_RANGE, y_train_inverse.shape[-1])
    yhat_inverse_time_step = yhat_inverse.reshape(-1, FORECAST_RANGE, yhat_inverse.shape[-1])
    
    residuals_time_step = y_train_inverse_time_step - yhat_inverse_time_step
    std_devs_time_step = np.std(residuals_time_step, axis=0).squeeze()
    
    last_values = scaled_data.values[-LOOK_BACK:]
    X_last = np.expand_dims(last_values, axis=0)
    X_last = torch.tensor(X_last, dtype=torch.float32).to(device)
    
    model.eval()
    with torch.no_grad():
        yhat_forecast = model(X_last).cpu().numpy()
    
    yhat_forecast = yhat_forecast.reshape(-1, len(forecast_indices))
    yhat_forecast_inverse, _ = inverse_transform(scaler, np.zeros_like(yhat_forecast), yhat_forecast, forecast_indices)
    
    yhat_forecast_inverse_last = yhat_forecast_inverse
    
    std_dev_constant = 0.67
    lower_bounds_original = yhat_forecast_inverse_last - std_dev_constant * yhat_forecast_inverse_last
    upper_bounds_original = yhat_forecast_inverse_last + std_dev_constant * yhat_forecast_inverse_last
    
    lower_bounds_residuals, upper_bounds_residuals = calculate_prediction_intervals_based_on_residuals(yhat_forecast_inverse_last, std_devs_time_step)
    
    last_date = combined_df.index[-1]
    date_range = pd.date_range(start=last_date, periods=FORECAST_RANGE, freq='W')
    
    plot_predictions_with_intervals(yhat_forecast_inverse_last, lower_bounds_residuals, upper_bounds_residuals, date_range, forecast_features)



 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\CNN_LSTM\training.py": 
 
 
import torch
from imports import *
from constants import *

def train_model(model, train_loader, criterion, optimizer, num_epochs, early_stopping_patience):
    best_val_loss = float('inf')
    patience = 0

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * X_batch.size(0)
        
        train_loss /= len(train_loader.dataset)
        
        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')
        
        if train_loss < best_val_loss:
            best_val_loss = train_loss
            patience = 0
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            patience += 1
            if patience >= early_stopping_patience:
                print("Early stopping")
                break

 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\CNN_LSTM\visualization.py": 
 
 
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
# imports.py
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pandas_datareader import data as pdr
from datetime import datetime, timedelta
import yfinance as yf
from scipy.stats import ttest_ind
import torch
from sklearn.preprocessing import MinMaxScaler
import matplotlib.dates as mdates
import optuna
import functools
import pandas as pd

from imports import *
from constants import *

import matplotlib.dates as mdates

def plot_predictions_with_intervals(y_pred, lower_bound_res, upper_bound_res, dates, feature_names, std_dev=0.67):
    # Ensure we're only using the last 13 points for the forecast horizon
    forecast_length = 13
    y_pred = y_pred[-forecast_length:]
    lower_bound_res = lower_bound_res[-forecast_length:]
    upper_bound_res = upper_bound_res[-forecast_length:]
    dates = dates[-forecast_length:]

    assert len(dates) == forecast_length, f"Expected {forecast_length} dates, got {len(dates)}"

    n_features = y_pred.shape[1]
    fig, axes = plt.subplots(n_features, 1, figsize=(12, 6*n_features), sharex=True)
    
    if n_features == 1:
        axes = [axes]
    
    z_score_percent = (stats.norm.cdf(std_dev) - stats.norm.cdf(-std_dev)) * 100
    
    for i, (ax, feature) in enumerate(zip(axes, feature_names)):
        y_pred_trimmed = y_pred[:, i]
        
        ax.plot(dates, y_pred_trimmed, label='Forecast', color='blue')
        ax.fill_between(dates, lower_bound_res[:, i], upper_bound_res[:, i], color='lightblue', alpha=0.4, label='Prediction Interval')
        
        ax.set_title(f'{feature} Forecast with Intervals\n(Based on residuals, ±{std_dev:.2f} std dev ≈ {z_score_percent:.2f}%)')
        
        ax.set_xlabel('Date')
        ax.set_ylabel('Value')
        
        ax.legend(loc='upper left')
        
        # Ensure x-axis shows exactly 13 dates
        ax.set_xticks(dates)
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    plt.gcf().autofmt_xdate()  # Rotate date labels
    plt.tight_layout()
    plt.savefig('forecast_intervals.png')
 
 
"C:\Users\User\Documents\wiki\wiki\dev\python\Python-Stock\code\CNN-TS\CNN_LSTM\__init__.py": 
 
 

